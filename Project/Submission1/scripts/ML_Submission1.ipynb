{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "cPpKVkIxPDux",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Multiple Linear Regression (MLR) of Synthetic Data with Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "-657yLidQRAm"
   },
   "outputs": [],
   "source": [
    "# Essential packages\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Statsmodels packages\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import OLSInfluence\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan, het_white\n",
    "\n",
    "# Scikit-learn packages\n",
    "from sklearn.linear_model import LinearRegression, Ridge, BayesianRidge, ARDRegression, Lasso, LassoLars, ElasticNet, HuberRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# Load data\n",
    "X_train = np.load(\"./data/X_train.npy\")\n",
    "y_train = np.load(\"./data/y_train.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iyMamvZJPZUl"
   },
   "source": [
    "## Outlier Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QIdO4KgLIC_Z"
   },
   "outputs": [],
   "source": [
    "def CreateOutlierRemovalPlots(sse_list, outliers_removed_list, residuals_list):\n",
    "    iterations = np.arange(1, len(sse_list) + 1)\n",
    "\n",
    "    # Plot 1: SSE over iterations\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(iterations, sse_list, marker='o')\n",
    "    plt.title(\"SSE vs Iterations\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Sum of Squared Errors (SSE)\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Plot 2: Number of Outliers Removed at each iteration\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(iterations, outliers_removed_list, marker='o', color='red')\n",
    "    plt.title(\"Number of Outliers Removed vs Iterations\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Number of Outliers Removed\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot 3: Studentized Residuals over Iterations\n",
    "    nrows = 2\n",
    "    ncols = int(np.ceil(len(residuals_list) / nrows))\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(15, 8))\n",
    "    axes = axes.ravel() # Flatten the axes array to make it easier to index\n",
    "\n",
    "    # Iterate over each iteration and plot the residuals\n",
    "    for i, residuals in enumerate(residuals_list):\n",
    "        axes[i].hist(residuals, bins=50, alpha=0.7, label=f\"Iteration {i+1}\")\n",
    "        axes[i].set_title(f\"Iteration {i+1}\")\n",
    "        axes[i].set_xlabel(\"Studentized Residuals\")\n",
    "        axes[i].set_ylabel(\"Frequency\")\n",
    "        axes[i].legend()\n",
    "        axes[i].grid(True)\n",
    "\n",
    "    # Remove any empty subplots (if any)\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    # Adjust the layout\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return\n",
    "\n",
    "def ScatterPlot(mask, y_train):\n",
    "    plt.scatter(range(len(y_train)), y_train, c=mask,\n",
    "                cmap='coolwarm', label='Inliers')\n",
    "    plt.xlabel(\"Sample index\")\n",
    "    plt.ylabel(\"Studentized Residuals\")\n",
    "    plt.show()\n",
    "\n",
    "    return\n",
    "\n",
    "def IterativeOutlierRemoval(X, y, threshold=3.1, max_iterations=20, tol=0.01):\n",
    "    X_with_const = sm.add_constant(X)\n",
    "\n",
    "    previous_sse = float('inf')\n",
    "\n",
    "    sse_list = []\n",
    "    tot_outliers = []\n",
    "    residuals_list = []\n",
    "    outliers_removed_list = []\n",
    "    X_train_outliers = []\n",
    "    y_train_outliers = []\n",
    "\n",
    "    iteration = 0\n",
    "\n",
    "    while iteration < max_iterations:\n",
    "        # Fit the linear regression model\n",
    "        model = sm.OLS(y, X_with_const).fit()\n",
    "\n",
    "        # Calculate studentized residuals\n",
    "        influence = OLSInfluence(model)\n",
    "        studentized_residuals = influence.resid_studentized_external\n",
    "\n",
    "        # Identify outliers based on studentized residuals\n",
    "        outliers = np.abs(studentized_residuals) > threshold\n",
    "        num_outliers = np.sum(outliers)\n",
    "        tot_outliers.append(num_outliers)\n",
    "        print(f\"Iteration {iteration + 1}: Detected {num_outliers} outliers.\")\n",
    "\n",
    "        # ScatterPlot(outliers, studentized_residuals)\n",
    "\n",
    "        if num_outliers == 0:\n",
    "            print(\"No more outliers detected. Stopping.\")\n",
    "            break\n",
    "\n",
    "        # Store outliers in vectors\n",
    "        # X_train_outliers.append(X_with_const[outliers, 1:])\n",
    "        # y_train_outliers.append(y[outliers])\n",
    "\n",
    "        # Store metrics for plotting\n",
    "        sse = np.sum((y - model.fittedvalues) ** 2)\n",
    "        sse_list.append(sse)\n",
    "        outliers_removed_list.append(num_outliers)\n",
    "        residuals_list.append(studentized_residuals)\n",
    "\n",
    "        # Remove outliers\n",
    "        X_with_const = X_with_const[~outliers]\n",
    "        y = y[~outliers]\n",
    "\n",
    "        # Refit the model and calculate new SSE\n",
    "        sse = np.sum((y - model.fittedvalues[~outliers]) ** 2)\n",
    "\n",
    "        # Stopping condition based on SSE improvement\n",
    "        if abs(previous_sse - sse) < tol:\n",
    "            print(f\"SSE change below tolerance ({tol}). Stopping at iteration {iteration + 1}.\")\n",
    "            break\n",
    "\n",
    "        previous_sse = sse  # Update SSE for the next iteration\n",
    "        iteration += 1\n",
    "\n",
    "    # CreateOutlierRemovalPlots(sse_list, outliers_removed_list, residuals_list)\n",
    "\n",
    "    # Return the cleaned dataset and outliers\n",
    "    # X_train_outliers = np.vstack(X_train_outliers)\n",
    "    # y_train_outliers = np.hstack(y_train_outliers)\n",
    "    X_train_cleaned = X_with_const[:, 1:]\n",
    "    y_train_cleaned = y\n",
    "    print(f\"Deleted a total of {np.sum(tot_outliers)} outliers.\")\n",
    "\n",
    "    return X_train_cleaned, y_train_cleaned\n",
    "\n",
    "# Perform outlier removal\n",
    "X_train_cleaned, y_train_cleaned = IterativeOutlierRemoval(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jVejwzLEsmYq"
   },
   "source": [
    "## Train-Test Split and Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Kdb0KgXsste"
   },
   "outputs": [],
   "source": [
    "def Standardize(vector):\n",
    "  mu = np.mean(vector, axis=0)\n",
    "  sigma = np.std(vector, axis=0)\n",
    "  standardized_vector = (vector - mu) / sigma\n",
    "\n",
    "  return standardized_vector, mu, sigma\n",
    "\n",
    "def Destandardize(standardized_vector, mu, sigma):\n",
    "  return standardized_vector * sigma + mu\n",
    "\n",
    "# Standardize cleaned data (without outliers)\n",
    "X_standardized, mu_X, sigma_X = Standardize(X_train_cleaned)\n",
    "\n",
    "# Split data into training and testing sets randomly\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_standardized, y_train_cleaned, train_size=0.75, random_state=120)\n",
    "\n",
    "# Set to True to train on all outlier free samples, otherwise use False\n",
    "if True:\n",
    "  X_train = X_standardized\n",
    "  y_train = y_train_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xm62FE5wooKm"
   },
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B62CR9CileDJ"
   },
   "outputs": [],
   "source": [
    "def TestHeteroskedasticity(X, y):\n",
    "    # Fit linear model\n",
    "    X = sm.add_constant(X)\n",
    "    model = sm.OLS(y, X).fit()\n",
    "\n",
    "    # Perform Breusch-Pagan test\n",
    "    lm, lm_pvalue, fvalue, f_pvalue = het_breuschpagan(model.resid, model.model.exog)\n",
    "\n",
    "    # Perform White test\n",
    "    white_test_results = het_white(model.resid, model.model.exog)\n",
    "    white_test_pvalue = white_test_results[1]\n",
    "\n",
    "    # Print test results\n",
    "    print(\"Breusch-Pagan Test:\")\n",
    "    print(f\"  LM Statistic: {lm:.6f}\")\n",
    "    print(f\"  LM p-value: {lm_pvalue:.6f}\")\n",
    "    print(f\"  F-value: {fvalue:.6f}\")\n",
    "    print(f\"  F p-value: {f_pvalue:.6f}\\n\")\n",
    "\n",
    "    print(\"White Test:\")\n",
    "    print(f\"  White Test p-value: {white_test_pvalue:.6f}\\n\")\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.scatter(model.fittedvalues, model.resid, edgecolors='k', facecolors='none')\n",
    "    plt.axhline(0, color='red', linestyle='--')\n",
    "    plt.xlabel('Fitted Values')\n",
    "    plt.ylabel('Residuals')\n",
    "    plt.title('Residuals vs Fitted Values')\n",
    "    plt.show()\n",
    "\n",
    "TestHeteroskedasticity(X_train_cleaned, y_train_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eu-1EMTsgWwc"
   },
   "source": [
    "## Test for Optimal Number of Folds in Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zo3_uY0JgFTw"
   },
   "outputs": [],
   "source": [
    "def TestNumberOfFolds(X, y, models, param_grids, folds_list):\n",
    "    # Dictionary to store results\n",
    "    results = {}\n",
    "\n",
    "    # Iterate over each model\n",
    "    for model_name, model, param_grid in zip(models.keys(), models.values(), param_grids):\n",
    "        print(f\"\\nTesting {model_name}\")\n",
    "        rmse_values = []\n",
    "        r2_values = []\n",
    "        alpha_values = []\n",
    "\n",
    "        # Test different fold values\n",
    "        for n_folds in folds_list:\n",
    "            print(f\"Testing with {n_folds} folds\")\n",
    "            # Initialize GridSearchCV\n",
    "            grid_search = GridSearchCV(\n",
    "                estimator=model,\n",
    "                param_grid=param_grid,\n",
    "                scoring=['neg_mean_squared_error', 'r2'],\n",
    "                cv=n_folds,\n",
    "                refit='neg_mean_squared_error',\n",
    "                verbose=0,\n",
    "                n_jobs = -1\n",
    "            )\n",
    "\n",
    "            # Fit the GridSearchCV to find the best parameter(s)\n",
    "            grid_search.fit(X, y)\n",
    "\n",
    "            # Extract the best parameter(s)\n",
    "            best_params = grid_search.best_params_\n",
    "            best_index = grid_search.best_index_\n",
    "\n",
    "            # Get metrics for plot\n",
    "            mean_rmse = np.sqrt(-grid_search.cv_results_['mean_test_neg_mean_squared_error'][best_index])\n",
    "            mean_r2 = grid_search.cv_results_['mean_test_r2'][best_index]\n",
    "            alpha = list(best_params.values())[0]\n",
    "\n",
    "            # Append results\n",
    "            rmse_values.append(mean_rmse)\n",
    "            r2_values.append(mean_r2)\n",
    "            alpha_values.append(alpha)\n",
    "\n",
    "        # Store results in dictionary\n",
    "        results[model_name] = {'folds': folds_list, 'rmse': rmse_values, 'r2': r2_values, 'alpha': alpha_values}\n",
    "\n",
    "    # Plot the results for each model\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(12, 3))\n",
    "\n",
    "    # RMSE Plot\n",
    "    ax[0].set_title('RMSE vs Number of Folds')\n",
    "    ax[0].set_xlabel('Number of Folds')\n",
    "    ax[0].set_ylabel('RMSE')\n",
    "    for model_name in results:\n",
    "        ax[0].plot(results[model_name]['folds'], results[model_name]['rmse'], label=model_name)\n",
    "    ax[0].legend()\n",
    "\n",
    "    # R^2 Plot\n",
    "    ax[1].set_title('R^2 vs Number of Folds')\n",
    "    ax[1].set_xlabel('Number of Folds')\n",
    "    ax[1].set_ylabel('R^2')\n",
    "    for model_name in results:\n",
    "        ax[1].plot(results[model_name]['folds'], results[model_name]['r2'], label=model_name)\n",
    "    ax[1].legend()\n",
    "\n",
    "    # Params\n",
    "    ax[2].set_title('Alpha vs Number of Folds')\n",
    "    ax[2].set_xlabel('Number of Folds')\n",
    "    ax[2].set_ylabel('Alpha')\n",
    "    for model_name in results:\n",
    "        ax[2].plot(results[model_name]['folds'], results[model_name]['alpha'], label=model_name)\n",
    "    ax[2].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Define the models and parameter grids\n",
    "models = {\n",
    "    'Ridge': Ridge(fit_intercept=True),\n",
    "    'Lasso': Lasso(fit_intercept=True)\n",
    "    #'Bayesian Ridge': BayesianRidge(fit_intercept=True)\n",
    "}\n",
    "\n",
    "param_grids = [\n",
    "    {'alpha': np.logspace(-4, 0, 100)},  # Ridge\n",
    "    {'alpha': np.logspace(-4, 0, 100)},  # Lasso\n",
    "    {\n",
    "        'alpha_init': np.linspace(0.1, 0.5, 10),\n",
    "        'alpha_1': np.logspace(-6, 0, 3),\n",
    "        'alpha_2': np.logspace(-6, 0, 3),\n",
    "        'lambda_1': np.logspace(-6, 0, 3),\n",
    "        'lambda_2': np.logspace(-6, 0, 3)\n",
    "    } # Bayesian Ridge\n",
    "]\n",
    "\n",
    "# List of fold values to test\n",
    "folds_list = np.arange(2, 21, 1)\n",
    "\n",
    "TestNumberOfFolds(X_train, y_train, models, param_grids, folds_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pa3VqWFjPhNT"
   },
   "source": [
    "## Hyperparameter Tuning and Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bDae95_dPkTj"
   },
   "outputs": [],
   "source": [
    "def CrossValidationTuning(X, y, model, param_grid, n_folds=5):\n",
    "    # Initialize GridSearchCV\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid,\n",
    "        scoring=['neg_mean_squared_error', 'r2'],\n",
    "        cv=n_folds,\n",
    "        refit='neg_mean_squared_error',\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # Fit the GridSearchCV to find the best parameter(s)\n",
    "    grid_search.fit(X, y)\n",
    "\n",
    "    # Extract the best parameter(s) and estimator\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_params = grid_search.best_params_\n",
    "    best_index = grid_search.best_index_\n",
    "\n",
    "    # Print best hyperparameter(s) found\n",
    "    print(f\"Best Hyperparameters: {best_params}\\n\")\n",
    "\n",
    "    # Print the metrics with respective std dev across folds\n",
    "    mean_rmse = np.sqrt(-grid_search.cv_results_['mean_test_neg_mean_squared_error'][best_index])\n",
    "    std_rmse = np.sqrt(grid_search.cv_results_['std_test_neg_mean_squared_error'][best_index])\n",
    "    mean_r2 = grid_search.cv_results_['mean_test_r2'][best_index]\n",
    "    std_r2 = grid_search.cv_results_['std_test_r2'][best_index]\n",
    "\n",
    "    print(f\"-> Best Model's Performance for {n_folds}-Fold CV\")\n",
    "    print(f\"Cross-Validated RMSE: {mean_rmse}\")\n",
    "    print(f\"RMSE STD Deviation: {std_rmse}\")\n",
    "    print(f\"Cross-Validated R^2: {mean_r2}\")\n",
    "    print(f\"R^2 STD Deviation: {std_r2}\\n\")\n",
    "\n",
    "    return best_model\n",
    "\n",
    "def RidgeModelTuning(X, y):\n",
    "  param_grid = {\n",
    "    'alpha': np.logspace(-4, 1, 100)\n",
    "  }\n",
    "\n",
    "  print(\"=== Ridge Model ===\")\n",
    "  target_model = Ridge(fit_intercept=True)\n",
    "  tuned_model = CrossValidationTuning(X, y, target_model, param_grid)\n",
    "\n",
    "  return tuned_model\n",
    "\n",
    "def BayesianRidgeModelTuning(X, y):\n",
    "    param_grid = {\n",
    "        'alpha_init': np.linspace(0.01, 1.0, 100),\n",
    "        'alpha_1': np.logspace(-6, 0, 3),\n",
    "        'alpha_2': np.logspace(-6, 0, 3),\n",
    "        'lambda_1': np.logspace(-6, 0, 3),\n",
    "        'lambda_2': np.logspace(-6, 0, 3)\n",
    "    }\n",
    "\n",
    "    print(\"=== Bayesian Ridge Model ===\")\n",
    "    target_model = BayesianRidge(fit_intercept=True)\n",
    "    tuned_model = CrossValidationTuning(X, y, target_model, param_grid)\n",
    "    tuned_model.fit(X,y)\n",
    "\n",
    "    return tuned_model\n",
    "\n",
    "def ARDRegressionModelTuning(X, y):\n",
    "    param_grid = {\n",
    "        'alpha_1': np.logspace(-6, 0, 10),\n",
    "        'alpha_2': np.logspace(-6, 0, 10),\n",
    "        'lambda_1': np.logspace(-6, 0, 10),\n",
    "        'lambda_2': np.logspace(-6, 0, 10)\n",
    "    }\n",
    "\n",
    "    print(\"=== ARD Regression Model ===\")\n",
    "    target_model = ARDRegression(fit_intercept=True)\n",
    "    tuned_model = CrossValidationTuning(X, y, target_model, param_grid)\n",
    "\n",
    "    return tuned_model\n",
    "\n",
    "def LassoModelTuning(X, y):\n",
    "  param_grid = {\n",
    "    'alpha': np.logspace(-4, 0, 100)\n",
    "  }\n",
    "\n",
    "  print(\"=== Lasso Model ===\")\n",
    "  target_model = Lasso(fit_intercept=True)\n",
    "  tuned_model = CrossValidationTuning(X, y, target_model, param_grid)\n",
    "\n",
    "  return tuned_model\n",
    "\n",
    "def LarsLassoModelTuning(X, y):\n",
    "    param_grid = {\n",
    "        'alpha': np.logspace(-4, 0, 100)\n",
    "    }\n",
    "\n",
    "    print(\"=== LarsLasso Model ===\")\n",
    "    target_model = LassoLars(fit_intercept=True)\n",
    "    tuned_model = CrossValidationTuning(X, y, target_model, param_grid)\n",
    "\n",
    "    return tuned_model\n",
    "\n",
    "def ElasticNetModelTuning(X, y):\n",
    "  param_grid = {\n",
    "    'alpha': np.logspace(-4, 1, 1000),\n",
    "    'l1_ratio': np.linspace(0.1, 0.9, 100), # L1 ratio: 0 (Ridge) to 1 (Lasso)\n",
    "  }\n",
    "\n",
    "  print(\"=== ElasticNet Model ===\")\n",
    "  target_model = ElasticNet(tol=1e-5)\n",
    "  tuned_model = CrossValidationTuning(X, y, target_model, param_grid)\n",
    "\n",
    "  return tuned_model\n",
    "\n",
    "\n",
    "def SVRModelTuning(X, y):\n",
    "  param_grid = {\n",
    "    'C': np.logspace(-2, 3, 100),\n",
    "    'epsilon': np.linspace(0, 1, 100)\n",
    "  }\n",
    "\n",
    "  print(\"=== SVR Model ===\")\n",
    "  target_model = SVR(kernel='linear')\n",
    "  tuned_model = CrossValidationTuning(X, y, target_model, param_grid)\n",
    "\n",
    "  return tuned_model\n",
    "\n",
    "def HuberModelTuning(X, y):\n",
    "    param_grid = {\n",
    "        'epsilon': np.linspace(1, 2, 10),\n",
    "        'alpha': np.logspace(-4, 1, 100)\n",
    "    }\n",
    "\n",
    "    print(\"=== Huber Regression Model ===\")\n",
    "    target_model = HuberRegressor()\n",
    "    tuned_model = CrossValidationTuning(X, y, target_model, param_grid)\n",
    "    tuned_model.fit(X,y)\n",
    "\n",
    "    return tuned_model\n",
    "\n",
    "def WLSModel(X, y):\n",
    "    X_with_constant = sm.add_constant(X)\n",
    "    ols_model = sm.OLS(y, X_with_constant).fit()\n",
    "    weights = 1 / np.square(ols_model.resid)\n",
    "    wls_model = sm.WLS(y, X_with_constant, weights=weights).fit()\n",
    "\n",
    "    print(\"=== WLS Model ===\")\n",
    "    print(\"WLS Summary:\", wls_model.summary())\n",
    "\n",
    "    return wls_model\n",
    "\n",
    "def GLSModel(X, y):\n",
    "    X_with_constant = sm.add_constant(X)\n",
    "    ols_model = sm.OLS(y, X_with_constant).fit()\n",
    "    sigma = np.diag(ols_model.resid**2) # Homoskedastic?\n",
    "    gls_model = sm.GLS(y, X_with_constant, sigma=sigma).fit()\n",
    "\n",
    "    print(\"=== GLS Model ===\")\n",
    "    print(gls_model.summary())\n",
    "\n",
    "    return gls_model\n",
    "\n",
    "# Baseline OLS model\n",
    "ols_model = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "# WLS linear model\n",
    "#wls_model = WLSModel(X_train, y_train)\n",
    "\n",
    "# GLS linear model\n",
    "#gls_model = GLSModel(X_train, y_train)\n",
    "\n",
    "# Perform CV on target models and extract the best hyperparameters\n",
    "tuned_ridge_model = RidgeModelTuning(X_train, y_train)\n",
    "tuned_bayesian_model = BayesianRidgeModelTuning(X_train, y_train)\n",
    "#tuned_ard_model = ARDRegressionModelTuning(X_train, y_train)\n",
    "tuned_lasso_model = LassoModelTuning(X_train, y_train)\n",
    "#tuned_lars_model = LarsLassoModelTuning(X_train, y_train)\n",
    "#tuned_elasticnet_model = ElasticNetModelTuning(X_train, y_train)\n",
    "#tuned_svr_model = SVRModelTuning(X_train, y_train)\n",
    "#tuned_huber_model = HuberModelTuning(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ukHXnH_bPk1-"
   },
   "source": [
    "## Performance Evaluation of the Best Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8-kOQPDJPpBy"
   },
   "outputs": [],
   "source": [
    "def PerformanceMetrics(tuned_model, X_test, y_test):\n",
    "  # Predict on the test data\n",
    "  y_test_pred = tuned_model.predict(X_test)\n",
    "\n",
    "  # Evaluate performance\n",
    "  test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "  test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "  test_r2 = r2_score(y_test, y_test_pred)\n",
    "  test_sse = np.sum((y_test - y_test_pred) ** 2)\n",
    "\n",
    "  # print(f\"Intercept: {tuned_model.intercept_}\")\n",
    "  # print(f\"Coefficients: {tuned_model.coef_}\\n\")\n",
    "\n",
    "  print(\"-> Error Metrics\")\n",
    "  print(f\" R^2: {test_r2:.6f}\")\n",
    "  print(f\"RMSE: {test_rmse:.6f}\")\n",
    "  print(f\" MAE: {test_mae:.6f}\")\n",
    "  print(f\" SSE: {test_sse:.6f}\\n\")\n",
    "\n",
    "  return\n",
    "\n",
    "print(\"=== OLS Model ===\")\n",
    "PerformanceMetrics(ols_model, X_standardized, y_train_cleaned)\n",
    "\n",
    "#print(\"=== WLS Model ===\")\n",
    "#PerformanceMetrics(wls_model, sm.add_constant(X_standardized), y_train_cleaned)\n",
    "\n",
    "#print(\"=== GLS Model ===\")\n",
    "#PerformanceMetrics(gls_model, sm.add_constant(X_standardized), y_train_cleaned)\n",
    "\n",
    "print(\"=== Ridge Model ===\")\n",
    "PerformanceMetrics(tuned_ridge_model, X_standardized, y_train_cleaned)\n",
    "\n",
    "print(\"=== BayesianRidge Model ===\")\n",
    "PerformanceMetrics(tuned_bayesian_model, X_standardized, y_train_cleaned)\n",
    "\n",
    "#print(\"=== ARD Regression Model ===\")\n",
    "#PerformanceMetrics(tuned_ard_model, X_standardized, y_train_cleaned)\n",
    "\n",
    "print(\"=== Lasso Model ===\")\n",
    "PerformanceMetrics(tuned_lasso_model, X_standardized, y_train_cleaned)\n",
    "\n",
    "#print(\"=== LarsLasso Model ===\")\n",
    "#PerformanceMetrics(tuned_lars_model, X_standardized, y_train_cleaned)\n",
    "\n",
    "#print(\"=== ElasticNet Model ===\")\n",
    "#PerformanceMetrics(tuned_elasticnet_model, X_standardized, y_train_cleaned)\n",
    "\n",
    "#print(\"=== SVR Model ===\")\n",
    "#PerformanceMetrics(tuned_svr_model, X_standardized, y_train_cleaned)\n",
    "\n",
    "#print(\"=== Huber Model ===\")\n",
    "#PerformanceMetrics(tuned_huber_model, X_standardized, y_train_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "drtDrg_aXI-E"
   },
   "source": [
    "## Save Best Model's Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q_Y9Un5lXIhl"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "X_total = np.load(\"./data/X_test.npy\")\n",
    "X_total_standardized, _, _ = Standardize(X_total)\n",
    "\n",
    "# Predict\n",
    "best_model = tuned_bayesian_model\n",
    "y_pred = best_model.predict(X_total_standardized)\n",
    "np.save('./y_pred.npy', y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0bFC9A4Yd4ZL"
   },
   "outputs": [],
   "source": [
    "y_pred_old = np.load(\"./data/y_pred_old.npy\")\n",
    "\n",
    "print(f\"MAE: {np.sum(np.abs(y_pred_old - y_pred)) / len(y_pred)}\")\n",
    "print(f\"SSE: {np.sum((y_pred_old - y_pred)**2)}\\n\")\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'y_pred_old': y_pred_old,\n",
    "    'y_pred_new': y_pred\n",
    "})\n",
    "\n",
    "print(comparison_df)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
